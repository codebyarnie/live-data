Below is the clean, formal clarification of how this should work in your architecture.

---

# 1. Candle Aggregator: Static, Code-Defined

Yes — the candle aggregator **should be statically coded** for the set of supported timeframes, not configured via YAML.

### Why?

1. **Cross-timeframe candle aggregation is deterministic math**, not user-configurable strategy logic.
2. Aggregators must be fast, predictable, and optimized.
3. YAML configuration introduces risk: mismatched TFs, unsupported TF cascades, or execution-order inconsistencies.

### Typical model

Hard-coded in code:

```python
SUPPORTED_TFS = ["1m", "5m", "15m", "1h"]
```

Individual aggregator workers:

* 1m → emits 1m and triggers 5m combiner
* 5m → emits 5m and triggers 15m combiner
* 15m → emits 15m and triggers 1h combiner

### Why this is correct

This is an **infrastructure concern**, not a trading logic concern.
It belongs to the *dataflow layer*, not the *strategy logic layer*.

---

# 2. Strategy and Indicator DAG: YAML-Configured

Correct — *strategies and indicators*, and the full DAG pipeline per symbol, **must be configured via YAML**.

This includes:

* which indicators exist
* their parameters
* which strategies exist
* their dependencies
* multi-TF usage
* node names
* outputs
* risk parameters

### Example (symbol pipeline YAML)

```yaml
symbol: ES
pipeline:
  - id: ema_fast
    type: EMA
    params:
      length: 20
      tf: 1m

  - id: ema_slow
    type: EMA
    params:
      length: 50
      tf: 1m
    depends_on:
      - ema_fast

  - id: swing_structure
    type: SwingPoints
    params:
      tf: 5m
    depends_on:
      - candles_5m

  - id: breakout_strategy
    type: LiquidityBreakout
    params:
      risk_multiple: 2.0
    depends_on:
      - ema_slow
      - swing_structure
```

This declarative definition is correct because:

* strategies/indicators are part of *trading logic*
* trading logic changes often
* you must be able to backtest different DAG layouts
* you must support per-symbol variations

The DAG runtime loads YAML configs → constructs the DAG → enforces topological order.

---

# 3. Summary

### **Candle Aggregator**

* Hardcoded
* Not YAML-configured
* Part of infrastructure
* Deterministic time-series math
* Runs the same for all symbols

### **Strategies / Indicators**

* YAML-configured
* Per-symbol or global configs
* Dynamic, user-editable
* Used to construct DAGs
* Changes do not require code deployment

This division is clean and correct.

---

If you want, I can provide a full `aggregator_1m.py` → `aggregator_5m.py` → `aggregator_15m.py` → `aggregator_1h.py` chain design, or a fully-expressed YAML schema for strategy/indicator nodes.

Yes — you **can and should** support multiple strategies in separate YAML files for the same instrument **while reusing indicator nodes across all strategies**.
This is both feasible and architecturally correct *if* you structure the DAG loader appropriately.

Below is the formal explanation of how to do it cleanly.

---

# 1. Core Principle

Indicators are **pure computational nodes**.
Strategies are **terminal nodes** that consume indicator outputs.

You can load any number of strategy YAML files that reference the same indicator IDs, and the DAG engine will naturally deduplicate indicator nodes.

### Example

* `es/base_indicators.yaml`
* `es/strategy_breakout.yaml`
* `es/strategy_reversion.yaml`

Both strategies can reference the same indicators:

```yaml
depends_on:
  - ema_20
  - ema_50
  - swing_points
```

The DAG engine will create:

* one `ema_20`,
* one `ema_50`,
* one `swing_points`,
* two strategy nodes.

All five nodes appear in the same DAG, but redundancy is eliminated.

---

# 2. How to Implement Indicator Reuse Formally

You need a **two-stage DAG build process**:

### Stage 1: Load and Normalize All YAML Files

Per instrument:

1. Read all YAML files in:
   `config/pipelines/ES/*.yaml`
2. Merge them into a single in-memory set of node definitions.
3. Validate uniqueness of node IDs.

### Stage 2: Deduplicate Node Definitions

Deduplication rule:

```
node_id is the unique identifier globally per instrument.
```

If another YAML defines a node with the same `id`, you:

* merge parameters if identical, or
* reject the configuration if conflicting.

This prevents multiple definitions of the same indicator with different params.

---

# 3. Example Directory Structure

```
config/
  pipelines/
    ES/
      indicators.yaml            # All shared indicator definitions
      strat_breakout.yaml        # Strategy DAG #1
      strat_mean_reversion.yaml  # Strategy DAG #2
      strat_liquidity.yaml       # Strategy DAG #3
```

### Example `indicators.yaml`

```yaml
indicators:
  - id: ema_20
    type: EMA
    params: { length: 20, tf: 1m }

  - id: ema_50
    type: EMA
    params: { length: 50, tf: 1m }

  - id: swing_points
    type: SwingPoints
    params: { tf: 5m }
```

### Example `strat_breakout.yaml`

```yaml
strategies:
  - id: breakout_liquidity
    type: LiquidityBreakout
    depends_on:
      - ema_20
      - swing_points
```

### Example `strat_reversion.yaml`

```yaml
strategies:
  - id: reversion_avg
    type: MeanReversion
    depends_on:
      - ema_20
      - ema_50
```

The DAG loader will produce:

* 3 indicator nodes
* 2 strategy nodes
* Single unified DAG → stable → topologically sorted

---

# 4. How the DAG Executor Benefits

### 1. Only one computation per indicator per event

Even if 10 strategies depend on `ema_20`, the DAG ensures:

* `ema_20` executes once
* all strategies consume its output

### 2. Lower resource usage

CPU cost stays low since indicators are the dominant load.

### 3. High composability

You can enable or disable strategies simply by adding/removing YAML files.

---

# 5. Mandatory Constraints (to avoid misconfiguration)

1. **Node IDs must be globally unique per symbol**
2. **Indicator nodes must not be redefined with different parameters**
3. **Strategies may depend on shared indicators but may not define their own duplicate indicators**
4. **DAG loader must reject cycles across files**
5. **All YAML files must contribute to a single DAG**

These rules maintain determinism.

---

# 6. Final Answer

**Yes, you can define multiple strategies in separate YAML files for the same instrument, and they can all reuse the same indicators.**
The DAG engine will merge the configurations, deduplicate indicators, and compute them once.

This is a correct and recommended architecture.

If you want, I can generate:

* the full YAML schema for multi-file DAG composition
* a reference DAG loader implementation outline
* example multi-strategy pipeline for ES/NQ
* validation rules for consistent reuse of nodes

Yes, it is entirely possible **if you design the architecture correctly**, but it requires a clear separation between:

1. **Indicator engines (per instrument)**
2. **Cross-instrument consumption layer (strategies)**

Below is the exact breakdown.

---

# 1. Indicators can belong to any instrument

Your indicator engine (or “candle engine + indicator engine”) naturally runs **per instrument**, because:

* OHLCV is per instrument
* Orderflow is per instrument
* Many indicators are mathematically meaningful only per instrument

So each instrument has its own indicator namespace, e.g.:

```
/indicators/ES
/indicators/NQ
/indicators/CL
```

Each namespace produces outputs into a sink (DB or stream), for example:

```
indicator_output:
  instrument: ES
  indicator: vwap
  value: ...
  timestamp: ...
```

---

# 2. Strategies can consume indicators across instruments

Strategies are **consumers**, not producers.

A strategy config can reference indicators from:

* its own instrument
* any other instrument
* even synthetic instruments (spreads, custom tickers, etc.)

Example YAML:

```yaml
strategy:
  name: "ES_spread_confirmation"
  instrument: ES

  inputs:
    es_vwap: indicators.ES.vwap
    nq_vwap: indicators.NQ.vwap
    es_delta: indicators.ES.delta
    nq_delta: indicators.NQ.delta
```

This means:

* The ES strategy listens to the ES indicator topics
* But ALSO subscribes to NQ indicator topics
* And uses them as features for signals

This is what hedge funds call **cross-asset feature loading** or **multi-asset factor ingestion**.

---

# 3. Technical requirement: indicators must be published to a shared transport

For this to work:

Most architectures use Kafka/NATS/Redis streams etc. to make the indicators available across instruments. Example:

```
indicators.ES.*
indicators.NQ.*
indicators.CL.*
```

Your strategy agent simply subscribes to the streams it needs.

---

# 4. Why it works cleanly

Because:

* The **indicator engines** remain fully instrument-isolated and stateless
* The **strategy engine** remains fully configurable and instrument-agnostic
* There is zero duplication of indicator calculations
* Latency is extremely low if you use structured streams

This is the preferred architecture in production quant systems.

---

# 5. Summary

**Yes**, you can absolutely consume indicators from different instruments in a strategy, as long as your indicator engines publish to a central transport and your strategy agent supports cross-topic subscription.

If you want, I can produce:

* The exact directory/repository structure
* Example full YAMLs
* Messaging topics layout (Kafka/NATS)
* How the strategy engine links the multi-instrument features

Just tell me.
Yes—an *indicator can absolutely consume several other indicators*.
And **it should still be called an indicator**, as long as it complies with the same semantics:

* It receives time-aligned inputs
* It produces a deterministic output
* It does not execute trading logic
* It is pure transformation, not a decision unit

This pattern is standard in quant systems and is usually called:

* **Composite indicators**
* **Derived indicators**
* **Feature transforms**
* **Meta-indicators**
* **Indicator chains**

All are legitimate “indicators” within the architecture.

Below is the precise classification.

---

# 1. Atomic vs Composite Indicators

### Atomic Indicator

Calculates directly from raw data (price, volume, orderflow, etc.)
Examples:

* EMA(20)
* VWAP
* Imbalance
* Delta

### Composite Indicator

Consumes one or more indicators as input.
Examples:

* EMA-of-VWAP
* Normalized Delta = Delta / VWAP deviation
* Relative Strength = EMA(close) / EMA(volume)
* Multi-TF Confirmation = HTF-EMA aligned with LTF-EMA

Architecturally, composite indicators are **still indicators**, not strategies.

---

# 2. Why they stay indicators (not strategies)

A strategy is defined by:

* It issues **signals**: buy/sell/flat
* It applies **logic**, thresholds, conditions
* It makes **decisions**

A composite indicator is defined by:

* It outputs **a numeric or categorical data series**
* It remains stateless except for its own calculation state
* It does not decide anything

So even if it combines 10 other indicators, as long as it only outputs features, it remains an indicator.

---

# 3. How it works in your architecture (clean design)

Your YAML could look like:

```yaml
indicators:
  ema20:
    type: ema
    input: candle.close
    period: 20

  vwap:
    type: vwap
    input: candle

  normalized_delta:
    type: formula
    inputs:
      delta: indicators.delta
      vwap: indicators.vwap
    expression: "delta / (vwap + 1e-9)"

  trend_strength:
    type: composite
    inputs:
      ema20: indicators.ema20
      ema50: indicators.ema50
    algorithm: trend_strength
```

This structure supports:

* Chaining
* Dependency resolution
* Multi-indicator ingestion
* Clean separation between feature generation and signal logic

---

# 4. Naming Recommendation

Use these categories:

### 1) Indicator

* Raw or derived
* Single or multi-input
* Numeric output

### 2) Feature

* A semantic name for a transformed indicator
* Could be identical to “indicator”

### 3) Strategy

* Consumes features/indicators
* Produces signals

You do *not* need another name unless you want to distinguish mathematically complex transforms, but in trading systems it is standard that "indicator" covers the whole hierarchy.

---

# 5. Summary

**Yes:** an indicator can take any number of indicators as input.
**No:** you do not need a different name.
**Call it a composite indicator or simply an indicator.**

If you want, I can show:

* How to structure composite indicator dependencies in YAML
* How to resolve dependency graphs
* How to deploy this on K3s with minimal coupling

Just tell me.

Yes, **an indicator can have several outputs**, and in a well-designed system this is not only allowed but often desirable. It simply means the indicator is producing a **vector feature** instead of a scalar feature.

This is standard in quantitative trading systems.

Below is the formal breakdown.

---

# 1. Multi-output indicators are normal

Examples:

### 1) Bollinger Bands

Outputs:

* upper_band
* middle_band
* lower_band

### 2) Market Profile

Outputs:

* poc
* value_area_high
* value_area_low
* distribution histogram

### 3) Orderflow analysis

Outputs:

* delta
* cumulative_delta
* aggressor_ratio
* bid_volume
* ask_volume

### 4) Regression-based indicators

Outputs:

* slope
* intercept
* residual
* r_squared

All of these are **multiple outputs from a single transformation unit**, and they are still indicators.

---

# 2. How to model multi-output indicators

### Option A — One indicator object with multiple named fields

This is the cleanest design.

Example YAML:

```yaml
indicators:
  market_profile:
    type: tpo
    input: candles
    parameters:
      session: RTH
```

Output is a structured result:

```
market_profile.poc
market_profile.value_high
market_profile.value_low
```

### Option B — Auto-split into separate indicator streams

Some pipelines prefer splitting outputs into flat streams:

```
indicator.market_profile.poc
indicator.market_profile.value_high
indicator.market_profile.value_low
```

This is useful for streaming systems (Kafka/NATS).

---

# 3. Why multi-output indicators are valuable

1. **Avoid recomputation**
   You do Market Profile once; do not compute POC three times.

2. **Logical cohesion**
   Outputs belong together and share the same computational context.

3. **Faster dataflow**
   Composite indicators or strategies can subscribe to only the fields they need.

4. **Cleaner dependency graph**
   You avoid bloating your config with unnecessary “fake” atomic indicators.

---

# 4. How strategies consume multi-output indicators

Example strategy config:

```yaml
strategy:
  name: "MP confirmation"
  instrument: ES

  inputs:
    poc: indicators.market_profile.poc
    vah: indicators.market_profile.value_high
    val: indicators.market_profile.value_low
    ema20: indicators.ema20
```

This is clean and scalable.

---

# 5. Summary

**Yes — an indicator can absolutely have multiple outputs.**
In fact, this is common and recommended for complex transforms.
Architecturally, you treat it as a **multi-field feature generator**.

If you want, I can show:

* The cleanest way to encode multi-output indicators in your YAML
* How to stream multi-output indicators over Kafka/NATS
* How to manage dependency resolution for multi-output indicators

Tell me what level you want.

1. Core Concept

The system is a Directed Acyclic Graph (DAG) of nodes.
Each node represents an indicator or strategy component.
Nodes may depend on:

Tick events

Candle events for any timeframe

Outputs from other indicators (any TF)

Mixed inputs (ticks + candles + other indicators)

When an event arrives (tick or candle), the system:

Determines which nodes are affected.

Computes each node once, in topological order.

Emits outputs (indicator values, strategy signals).

This approach guarantees deterministic, non-duplicated, race-free execution.

2. Node Model (Indicator Definition)

Each indicator is defined as:

id: string
inputs: List[InputRef]       # tick | candle(TF) | indicator(id)
compute(state, inputs) -> output
output_schema: arbitrary
state: persistent state object


Where InputRef can be:

Tick
Candle(TF)
Indicator(ID)


This allows arbitrary fan-in, cross-TF, and cross-indicator dependencies.

3. Global DAG and Coordinator

The coordinator holds:

Node registry (id → node)

Dependency graph (edges between nodes)

Reverse dependency graph (for impact propagation)

Topological order (pre-computed at startup)

Execution engine (runs nodes in correct order)

Event router (determines which nodes to recompute)

No indicator consumes external events directly.
Only the coordinator interacts with NATS or other brokers.

4. Event Flow
Step 1 — Event arrives

Example: Candle(5m)

Step 2 — Mark impacted nodes

Using the reverse dependency graph:

impacted = all nodes that depend directly or transitively on Candle(5m)

Step 3 — Sort by DAG order

The coordinator filters the precomputed topological order:

exec_order = [node for node in topo_order if node in impacted]

Step 4 — Compute nodes

Each node:

Fetches the latest values from its inputs

Calls its compute()

Updates its state

Stores its output in the shared result store

Each node is executed exactly once per event, even if multiple parents updated.

5. Multi-Timeframe Handling

Define candle streams:

Candle(1m)
Candle(5m)
Candle(15m)
Candle(30m)
...


Aggregation chain (1m → 5m → 15m → etc.) is either:

done externally before reaching the DAG, or

modeled as nodes inside the DAG.

Indicators simply declare:

dependencies on the TFs they use

dependencies on other indicators (regardless of TF)

This generalizes naturally with DAG semantics.

6. Cross-Indicator Dependencies (Explicit)

If Indicator B depends on Indicator A:

inputs: [
    Indicator("A"),
    Candle("5m")
]


The DAG ensures:

Indicator A is computed before Indicator B

Indicator B triggers when A changes

No recomputation storms occur

Example:

A → B → D
 \
  \→ C → D


All valid and automatically resolved.

7. State Management

Each node holds its own state:

rolling windows

indicator buffers

internal counters

multi-TF caches

cooldown logic

smoothing filters

This state is passed into compute() and updated in place.

No global mutable state exists.

8. Integration with NATS or Other Brokers

Coordinator acts as the boundary:

Inbound:

Ticks

Candle(1m), Candle(5m), …

Outbound:

Indicator updates → indicators.<id>

Strategy outputs → strategy.signals.<symbol>

Dashboard events → dashboard.stream.<channel>

Indicators themselves do not subscribe directly to NATS.

9. Correctness Guarantees

The system ensures:

Deterministic execution order

Zero duplicated computations

Zero race conditions

Consistent multi-TF alignment

Strict event coherence (no next tick until current DAG done)

Safe fan-in and fan-out

Arbitrary expansion/convergence patterns supported

10. Step-by-Step Implementation Guide
Step 1 — Define the Node Schema
class Node:
    id: str
    inputs: List[InputRef]
    state: dict
    def compute(self, inputs, state) -> output: ...

Step 2 — Declare Indicators

Use a YAML/JSON or Python config:

A:
  inputs: [Tick, Candle(1m)]
B:
  inputs: [Indicator:A, Candle(5m)]
C:
  inputs: [Candle(1m), Candle(15m)]
D:
  inputs: [Indicator:B, Indicator:C]

Step 3 — Build the DAG

Register nodes

Validate no cycles

Build adjacency list and reverse edges

Compute topological order

Step 4 — Create Event Router

For each event type, precompute which nodes depend on it.

Step 5 — Implement Executor

On event:

Determine impacted nodes

Determine execution order

Fetch cached inputs

Compute nodes

Store outputs

Publish results via NATS

Step 6 — Add Persistence for Node State

Use:

in-memory for low latency

snapshotting for recovery

optional Redis/RocksDB buffer for replay/resilience

Step 7 — Add Strategy Layer

Strategies are simply nodes farther down in the DAG.

Step 8 — Add Dashboard/WebSocket Layer

A final DAG branch can publish aggregated indicator streams.


Recommended repository strategy (mono-repo vs multi-repo)

Recommended directory layout

Deployment structure on K3s

Scaling/operational implications

1. Recommended Repository Strategy

Use a hybrid monorepo, not many small repos.

Why not many repos?

If you split:

indicators

DAG engine

NATS consumers

strategies

aggregator

dashboard feed

into separate repos, you will suffer from:

versioning drift

dependency mismatch

complicated CI/CD

difficult local development

no global typing of indicator outputs

This is a tight domain where all components must evolve together.
You want atomic commits that update:

engine + 5 indicators + 2 strategies
in one change-set.

Why not a giant monorepo with everything?

You should separate things that:

scale differently

are owned by different teams

change at different cadences

run outside the trading engine core

So: hybrid monorepo structure

Use one repository for the entire trading engine, including:

tick ingestion

candle aggregation

DAG compute engine

indicators

strategies

NATS input/output adapters

dashboard publishers

But put infrastructure, ops tooling, dashboards, and monitoring in separate repos.

2. Recommended Repo Layout (Monorepo)

Below is a clean production-level layout.

repo-root/
│
├── engine/                         # Core DAG compute engine
│   ├── dag/
│   ├── scheduler/
│   ├── state/
│   └── runtime/
│
├── indicators/                     # Each indicator is a node plugin
│   ├── moving_average/
│   ├── vwap/
│   ├── imbalance/
│   ├── footprint/
│   └── ... (many)
│
├── strategies/                     # Strategy nodes living atop the DAG
│   ├── breakout/
│   ├── mean_reversion/
│   └── liquidity_hunt/
│
├── dataflow/                       # Event producers & consumers
│   ├── ingestion/                  # tick → NATS
│   ├── candle_aggregation/         # 1m → 5m → ...
│   ├── adapters/                   # NATS in/out, WebSockets, APIs
│   └── publishers/
│
├── config/                         # Declarative DAG definitions
│   ├── indicators.yaml
│   ├── strategies.yaml
│   └── pipelines/
│       └── symbol_template.yaml
│
├── infra/                          # K3s deploy manifests (OPTIONAL)
│   ├── helm/
│   ├── k8s/
│   └── kustomize/
│
├── tests/                          # Unit, integration, deterministic-replay
│   ├── replay/
│   ├── fuzz/
│   └── e2e/
│
└── tools/                          # Developer CLI, utilities
    └── dagviz/                     # Graph visualization tool

Why this structure?

Indicators and strategies are plugins inside the same repo.

DAG engine is isolated and testable.

Data ingestion/aggregation is modular.

K3s manifests can live here OR in a separate “infra repo”.

3. K3s Deployment Model

You do not deploy every indicator as a microservice.
You deploy per-symbol DAG executors or per-symbol groups.

Typical deployment:
Deployment: tick-collector
Deployment: candle-aggregator
Deployment: engine-executor (N replicas based on symbols)
Deployment: dashboard-streamer
StatefulSet: timeseries-db (optional)
Service: nats

Scaling pattern (very important)

You scale by symbols, not components:

engine-executor --symbol=ES
engine-executor --symbol=NQ
engine-executor --symbol=CL


Each executor runs:

the DAG engine

all indicators

all strategies
for a given instrument.

This keeps:

state local

latency minimal

ordering strict

no cross-node chatter

no cross-service races

This is exactly how HFT/prop desks structure computation.

4. How Many Repos?
Recommended split (3 repos total):

Repo 1 — trading-engine (monorepo)
Contains:

ingestion

aggregation

DAG execution engine

indicators

strategies

NATS adapters

dashboard publishers

This is your main repo — where your domain logic lives.

Repo 2 — infra (K3s + CI/CD + Helm + observability)
Contains:

helm charts

k8s manifests

logging stack (Loki / Promtail)

metrics (Prometheus + Grafana)

deploying NATS / cert-manager / secrets

Tekton/Argo workflows or GitHub Actions

RBAC, network policies

Reason:
Infra evolves on a completely different cycle than indicators or strategies.

Repo 3 — dashboard/UI
Contains:

WebSockets client

UI for indicators, signals, performance

React/Vue/Svelte code

Grafana dashboards or custom dashboards

Reason:
Frontend/UI has separate dependencies and release cadence.

5. Summary Table
Component	Repo	Reason
DAG Engine	trading-engine	Core, must version lock with indicators
Indicators	trading-engine	Plugin-style, high coupling
Strategies	trading-engine	Locked to engine version
NATS interfaces	trading-engine	Directly tied to dataflow
Candle aggregation	trading-engine	Shared types & state
Tick ingestion	trading-engine	Shared types
K3s Manifests	infra	DevOps lifecycle, separated
Helm Charts	infra	Ops-managed
Observability stack	infra	Ops-managed
Dashboards/UI	dashboard	Independent lifecycle
6. Optional Variation (If You Want Maximum Modularity)

1. The DB Sink Is Part of the Engine Layer

Your Engine layer contains all deterministic, single-tick–ordered consumers that must process every event exactly once.
This includes:

Indicator services

Candle aggregators

Strategy evaluators

Database sink (tick → candle → indicator → strategy outputs → persistent storage)

Because the DB sink must receive the same ordered, lossless event stream as indicators and strategies, it belongs in the Engine rather than in the UI/API or infrastructure layer.

2. But the DB Sink Should Be Its Own Microservice

Do not embed DB writes into other services.

Reasons:

Clear separation of concerns

Easier schema evolution

Recoverable and replayable (if NATS JetStream is used)

Does not impact latency of indicator computation

Isolated failure domain (DB outage does not break indicators/strategies)

Engine layout (conceptual)
engine/
  aggregator-1m/
  aggregator-5m/
  indicator-rsi/
  indicator-delta/
  indicator-structure/
  strategy-meanrev/
  strategy-breakout/
  sink-db/  <-- here


The sink-db subscribes to topics like:

ticks.raw
candles.1m
candles.5m
indicators.*
strategies.signals


and writes to Postgres, ClickHouse, TimescaleDB, or whatever backend you use.

3. Why not place DB sink in the ingestion layer?

Because ingestion is stateless and fire-and-forget.
Persistence belongs on the deterministic, ordered side where events have been validated.

4. Why not place DB sink in the UI layer?

Because it serves operational data and must keep up with high throughput streams.

Final Recommendation

Place the DB sink inside the Engine, as a dedicated microservice, deployed independently, but fed from the same event streams as indicators and strategies.

If you want, I can update the README.md in the canvas to explicitly include the DB sink component and its responsibilities.